{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "__1. Image Input Data :-__\n",
    "\n",
    "Let’s assume we have a dataset of gray scale images. Each image has the same size of 32 pixels wide and 32 pixels high, and pixel values are between 0 and 255, e.g. a matrix of 32 X 32 X 1 or 1,024 pixel values. Image input data is expressed as a 3-dimensional matrix of width X height X channels. If we were using color images in our example, we would have 3 channels for the red, green and blue pixel values, e.g. 32 X 32 X 3.\n",
    "\n",
    "__2. Convolutional Layer :-__\n",
    "\n",
    "We define a convolutional layer with 10 filters and a receptive field 5 pixels wide and 5 pixels high and a stride length of 1. Because each filter can only get input from (i.e. see) 5 X 5 (25) pixels at a time, we can calculate that each will require 25 + 1 input weights (plus 1 for the bias input). Dragging the 5 X 5 receptive field across the input image data with a stride width of 1 will result in a feature map of 28 X 28 output values or 784 distinct activations per image.\n",
    "We have 10 filters, so that is 10 different 28 X 28 feature maps or 7,840 outputs that will be created for one image. Finally, we know we have 26 inputs per filter, 10 filters and 28 X 28 output values to calculate per filter, therefore we have a total of 26 X 10 X 28 X 28 or 203,840 connections in our convolutional layer, we want to phrase it using traditional neural network nomenclature. Convolutional layers also make use of a nonlinear transfer function as part of activation and the rectifier activation function is the popular default to use.\n",
    "\n",
    "__3. Pool Layer :-__\n",
    "\n",
    "We define a pooling layer with a receptive field with a width of 2 inputs and a height of 2 inputs. We also use a stride of 2 to ensure that there is no overlap. This results in feature maps that are one half the size of the input feature maps. From 10 different 28 X 28 feature maps as input to 10 different 14 X 14 feature maps as output. We will use a max() operation for each receptive field so that the activation is the maximum input value.\n",
    "\n",
    "__4. Fully Connected Layer :-__\n",
    "Finally, we can flatten out the square feature maps into a traditional flat fully connected layer. We can define the fully connected layer with 200 hidden neurons, each with 10 ⇥ 14 ⇥ 14 input connections, or 1,960 + 1 weights per neuron. That is a total of 392,200 connections and weights to learn in this layer. We can use a sigmoid or softmax transfer function to output probabilities of class values directly.\n",
    "\n",
    "\n",
    "## Best Practices in CNN \n",
    "\n",
    "1. __Receptive Field Size__: The patch should be as small as possible, but large enough to see features in the input data. It is common to use 3 X 3 on small images and 5 X 5 or 7 X 7 and more on larger image sizes.\n",
    "2. __Stride Width__: Use the default stride of 1. It is easy to understand and you don’t need padding to handle the receptive field falling off the edge of your images. This could be increased to 2 or larger for larger images.\n",
    "3. __Number of Filters__: Filters are the feature detectors. Generally fewer filters are used at the input layer and increasingly more filters used at deeper layers.\n",
    "4. __Padding__: Set to zero and called zero padding when reading non-input data. This is useful when you cannot or do not want to standardize input image sizes or when you want to use receptive field and stride sizes that do not neatly divide up the input image size.\n",
    "5. __Pooling__: Pooling is a destructive or generalization process to reduce overfitting. Receptive field size is almost always set to 2 X 2 with a stride of 2 to discard 75% of the activations from the output of the previous layer.\n",
    "6. __Pattern Architecture__: It is common to pattern the layers in your network architecture. This might be one, two or some number of convolutional layers followed by a pooling layer. This structure can then be repeated one or more times. Finally, fully connected layers are often only used at the output end and may be stacked one, two or more deep.\n",
    "7. __Dropout__: CNNs have a habit of overfitting, even with pooling layers. Dropout should be used such as between fully connected layers and perhaps after pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import from keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Conv2D\n",
    "\n",
    "#import from matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the MNIST dataset\n",
    "(X_train, y_train) , (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting few examples\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first build a multilayer perceptron model.\n",
    "\n",
    "## Baseline model using Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the MNIST dataset\n",
    "(X_train, y_train) , (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the 28 X 28 image to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype(\"float32\")\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of the input variables from 0-255 to 0-1.\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model\n",
    "def baseline_model():\n",
    "    #Create Model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim = num_pixels, kernel_initializer = 'normal', activation = 'relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer = 'normal', activation = 'softmax'))\n",
    "    # Compile Model\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.2798 - accuracy: 0.9208 - val_loss: 0.1412 - val_accuracy: 0.9572\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.1116 - accuracy: 0.9675 - val_loss: 0.0919 - val_accuracy: 0.9704\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.0717 - accuracy: 0.9798 - val_loss: 0.0781 - val_accuracy: 0.9777\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.0503 - accuracy: 0.9858 - val_loss: 0.0742 - val_accuracy: 0.9772\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.0372 - accuracy: 0.9893 - val_loss: 0.0686 - val_accuracy: 0.9786\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.0269 - accuracy: 0.9928 - val_loss: 0.0619 - val_accuracy: 0.9807\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.0205 - accuracy: 0.9950 - val_loss: 0.0611 - val_accuracy: 0.9811\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0621 - val_accuracy: 0.9806\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.0578 - val_accuracy: 0.9812\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.0581 - val_accuracy: 0.9812\n",
      "Baseline Error: 1.88%\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), \n",
    "          epochs = 10, batch_size = 200, verbose = 2)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Baseline Error: %.2f%%\" %(100- scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error from baseline model is 1.88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# loading the MNIST dataset\n",
    "(X_train, y_train) , (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(\"float32\")\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of the input variables from 0-255 to 0-1.\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our CNN structure\n",
    "\n",
    "1. The first hidden layer is a convolution layer. The layer has 32 feature map, which with the size of 5 X 5 and a rectifier activation function.\n",
    "\n",
    "2. A pooling layer with a pool size of 2 X 2.\n",
    "\n",
    "3. The next layer is a regularization layer using dropout.\n",
    "\n",
    "4. Next is a layer that converts the 2D matrix data to a vector called Flatten.\n",
    "\n",
    "5. Next a fully connected layer with 128 neurons and rectifier activation function is used.\n",
    "\n",
    "6. The output layer has 10 neurons for the 10 classes and a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5,5), padding='valid', input_shape = (28,28,1),\n",
    "                     activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 43s - loss: 0.2515 - accuracy: 0.9273 - val_loss: 0.0949 - val_accuracy: 0.9721\n",
      "Epoch 2/10\n",
      " - 45s - loss: 0.0782 - accuracy: 0.9764 - val_loss: 0.0524 - val_accuracy: 0.9829\n",
      "Epoch 3/10\n",
      " - 43s - loss: 0.0532 - accuracy: 0.9844 - val_loss: 0.0417 - val_accuracy: 0.9861\n",
      "Epoch 4/10\n",
      " - 43s - loss: 0.0418 - accuracy: 0.9876 - val_loss: 0.0381 - val_accuracy: 0.9873\n",
      "Epoch 5/10\n",
      " - 42s - loss: 0.0343 - accuracy: 0.9896 - val_loss: 0.0401 - val_accuracy: 0.9872\n",
      "Epoch 6/10\n",
      " - 43s - loss: 0.0277 - accuracy: 0.9914 - val_loss: 0.0369 - val_accuracy: 0.9882\n",
      "Epoch 7/10\n",
      " - 44s - loss: 0.0237 - accuracy: 0.9924 - val_loss: 0.0336 - val_accuracy: 0.9892\n",
      "Epoch 8/10\n",
      " - 45s - loss: 0.0189 - accuracy: 0.9940 - val_loss: 0.0322 - val_accuracy: 0.9899\n",
      "Epoch 9/10\n",
      " - 45s - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.0323 - val_accuracy: 0.9898\n",
      "Epoch 10/10\n",
      " - 45s - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0368 - val_accuracy: 0.9891\n",
      "Baseline Error: 1.09%\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 10, batch_size = 200, verbose = 2)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "print(\"Baseline Error: %.2f%%\" %(100- scores[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error has reduced to 1.09%, which is lesser than what we got with multilayer perceptron model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
